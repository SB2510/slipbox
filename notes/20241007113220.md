# When doesn an algorithm have linear time? 
When just going over a running variable once, or twice, but only multiplying the input n and not e.g. squaring it.
The running variable grows linearly [[20241007120026]]. O(n) [[20241007114148]] $O(n)=n$ (because multiplicative constants can be left out).

e.g. 
```c
void function(int n){
	for(int i =0; i < n; i++){
    	// Do something here
    }
}
```

#ep1 #algorithms