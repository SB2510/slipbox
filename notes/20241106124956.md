# What is entropy?
Entropy is the weighted mean information content of all symbols in a message [[20241106123319]]. This can be calculated with the following formula
using a finite sum and a logarithmic function [[20241007115904]] of base 2:

$H = -\sum_{i \in \mathcal{I}} p_i * log_2(p_i)$


#gds #informationtheory